{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b29d95d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2 as cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip, \\\n",
    "                RandomVerticalFlip, ColorJitter, RandomRotation, GaussianBlur, RandomCrop, RandomPerspective, RandomAffine\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "sys.path.append('C:\\\\Users\\\\LuCo\\\\Documents\\\\repos\\\\Depth-Anything-V2')\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "from metric_depth.dataset.transform import Resize, NormalizeImage, PrepareForNet, Crop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fda8a4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Parameters\n",
    "# ------------------------------\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DATA_DIR = os.path.join(os.getcwd(), 'data', \"train\", \"train\")\n",
    "CHECKPOINT_DIR = os.path.join(os.getcwd(), 'checkpoints')\n",
    "BEST_CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, 'best_depth_anything_v2.pth')\n",
    "BACKUP_CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, 'depth_anything_v2.pth')\n",
    "PATIENCE_CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, 'patience_depth_anything_v2_vitg.pth')\n",
    "ENCODER = 'vitl'\n",
    "NUM_EPOCHS = 12\n",
    "LOG_DIR = os.path.join(os.getcwd(), 'logs')\n",
    "RESULT_DIR = os.path.join(os.getcwd(), 'results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffe81878",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthDataset(Dataset):\n",
    "    def __init__(self, root, mode='train', size = (518, 518)):\n",
    "        self.root = root\n",
    "        self.mode = mode\n",
    "        self.size = size\n",
    "\n",
    "        self.rgb_paths = sorted(glob.glob(os.path.join(root, '*_rgb.png')))\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            self.depth_paths = [p.replace('_rgb.png', '_depth.npy') for p in self.rgb_paths]\n",
    "\n",
    "        net_w, net_h = size\n",
    "\n",
    "\n",
    "        self.transform = Compose([\n",
    "            Resize(width=net_w, height=net_h, \n",
    "                   resize_target=True if mode == 'train' else False, \n",
    "                   keep_aspect_ratio=True, \n",
    "                   ensure_multiple_of=14, \n",
    "                   resize_method=\"lower_bound\",\n",
    "                   image_interpolation_method=cv2.INTER_CUBIC),\n",
    "            NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            PrepareForNet(),\n",
    "        ] + ([Crop(size[0])] if mode == 'train' else []))\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_paths)\n",
    "    \n",
    "\n",
    "    def horizontal_flip(self, image, depth, p=0.3):\n",
    "        if np.random.rand() < p:\n",
    "            image = np.fliplr(image).copy()\n",
    "            depth = np.fliplr(depth).copy()\n",
    "        return image, depth\n",
    "    \n",
    "    def brightness_jitter(self, image, p = 0.25):\n",
    "        if np.random.rand() < p:\n",
    "            hsv = cv2.cvtColor((image * 255).astype(np.uint8), cv2.COLOR_RGB2HSV)\n",
    "            hsv = np.array(hsv, dtype=np.float32)\n",
    "            hsv[..., 2] *= 0.8 + 0.4 * np.random.rand()\n",
    "            hsv[..., 2] = np.clip(hsv[..., 2], 0, 255)\n",
    "            rgb = cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)\n",
    "            image = rgb.astype(np.float32) / 255.0\n",
    "            \n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load RGB image\n",
    "        rgb = cv2.imread(self.rgb_paths[idx])\n",
    "        rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB) / 255.0\n",
    "        \n",
    "        # Load depth map\n",
    "        if self.mode == 'train':\n",
    "            depth = np.load(self.depth_paths[idx])\n",
    "\n",
    "        # Apply data augmentation (e.g. horizontal flip)\n",
    "        if self.mode == 'train':\n",
    "            rgb, depth = self.horizontal_flip(rgb, depth, p=0.1)\n",
    "            rgb = self.brightness_jitter(rgb, p=0.1)\n",
    "\n",
    "            sample = self.transform({'image': rgb, 'depth': depth})\n",
    "        \n",
    "        if self.mode == 'test':\n",
    "            sample = self.transform({'image': rgb})\n",
    "            sample['image'] = torch.from_numpy(sample['image'])\n",
    "\n",
    "        if self.mode == 'test':\n",
    "            sample['filenames'] = self.rgb_paths[idx]\n",
    "            \n",
    "        if self.mode == 'train':\n",
    "            sample['depth'] = torch.from_numpy(sample['depth'])\n",
    "\n",
    "            sample['valid_mask'] = (torch.isnan(sample['depth']) == 0)\n",
    "            sample['depth'][sample['valid_mask'] == 0] = 0\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f7e2099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleInvariantRMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaleInvariantRMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.squeeze()\n",
    "        target = target.squeeze()\n",
    "\n",
    "        pred_flat = pred.view(pred.size(0), -1)\n",
    "        target_flat = target.view(target.size(0), -1)\n",
    "\n",
    "        log_pred = torch.log(pred_flat + 1e-8)\n",
    "        log_target = torch.log(target_flat + 1e-8)\n",
    "\n",
    "        delta = log_target - log_pred\n",
    "\n",
    "        alpha = delta.mean(dim=1, keepdim=True)\n",
    "\n",
    "        loss = ((-delta + alpha) ** 2).mean(dim=1).sqrt()\n",
    "\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfae977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def compute_losses(self, inputs, outputs):\n",
    "        \"\"\"Compute the reprojection and smoothness losses for a minibatch\n",
    "        \"\"\"\n",
    "        losses = {}\n",
    "        total_loss = 0\n",
    "\n",
    "        for scale in self.opt.scales:\n",
    "            loss = 0\n",
    "\n",
    "            disp = outputs[(\"disp\", scale)]\n",
    "            color = inputs[(\"color\", 0, scale)]\n",
    "            target = inputs[(\"color\", 0, scale)]\n",
    "\n",
    "            for frame_id in self.opt.frame_ids[1:]:\n",
    "                pred = outputs[(\"color\", frame_id, scale)]\n",
    "                pred_new = outputs[(\"color_new\", frame_id, scale)]\n",
    "\n",
    "                outputs[(\"reprojection_losses_vitual\", frame_id, scale)] = self.compute_reprojection_loss(pred, target)\n",
    "                loss += outputs[(\"reprojection_losses_vitual\", frame_id, scale)].mean()\n",
    "\n",
    "                outputs[(\"reprojection_losses_new\", frame_id, scale)] = self.compute_reprojection_loss(pred_new, target)\n",
    "                loss += outputs[(\"reprojection_losses_new\", frame_id, scale)].mean()\n",
    "\n",
    "\n",
    "            if self.opt.disable_plane_smoothness:\n",
    "                mean_disp = disp.mean(2, True).mean(3, True)\n",
    "                norm_disp = disp / (mean_disp + 1e-7)\n",
    "                smooth_loss = get_smooth_loss(norm_disp, color)\n",
    "            else:\n",
    "                mean_coeff = outputs[(\"coeff\", scale)].abs().mean(2, True).mean(3, True)\n",
    "\n",
    "                norm_coeff = outputs[(\"coeff\", scale)] / (mean_coeff + 1e-7)\n",
    "                smooth_loss = get_smooth_loss(norm_coeff, color)\n",
    "\n",
    "            loss += self.opt.smoothness_weight / (2 ** scale) * smooth_loss\n",
    "            losses[\"smooth_loss/{}\".format(scale)] = smooth_loss\n",
    "\n",
    "            point3D = outputs[(\"cam_points\", 0, scale)][:, :3, ...]\n",
    "            mean_depth = outputs[(\"depth\", 0, scale)].mean(2, True).mean(3)\n",
    "            norm_point3D = point3D/(mean_depth + 1e-7)\n",
    "\n",
    "            if not self.opt.disable_plane_regularization:\n",
    "                key = (\"plane_keysets\", 0, scale)\n",
    "                if key in inputs:\n",
    "                    plane_loss = get_plane_loss(inputs[key], norm_point3D)\n",
    "                    loss += self.opt.plane_weight * plane_loss\n",
    "                    losses[\"plane_loss/{}\".format(scale)] = plane_loss\n",
    "                else:\n",
    "                    print(f\"[Warning] Missing key {key} in inputs during compute_losses.\")\n",
    "\n",
    "            if not self.opt.disable_line_regularization:\n",
    "                key = (\"line_keysets\", 0, scale)\n",
    "                if key in inputs:\n",
    "                    line_loss = get_line_loss(inputs[key], norm_point3D)\n",
    "                    loss += self.opt.line_weight * line_loss\n",
    "                    losses[\"line_loss/{}\".format(scale)] = line_loss\n",
    "                else:\n",
    "                    print(f\"[Warning] Missing key {key} in inputs during compute_losses.\")\n",
    "\n",
    "            losses[\"loss/{}\".format(scale)] = loss\n",
    "            total_loss += loss\n",
    "\n",
    "        total_loss /= self.num_scales\n",
    "        losses[\"loss\"] = total_loss\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe4059a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthGradientLoss(nn.Module):\n",
    "    def __init__(self, edge_aware=True):\n",
    "        super(DepthGradientLoss, self).__init__()\n",
    "        self.edge_aware = edge_aware\n",
    "\n",
    "    def forward(self, pred, image):\n",
    "        pred_dx = torch.abs(pred[ :, :, :-1] - pred[ :, :, 1:])\n",
    "        pred_dy = torch.abs(pred[ :, :-1, :] - pred[ :, 1:, :])\n",
    "\n",
    "        if self.edge_aware:\n",
    "            image_dx = torch.mean(torch.abs(image[ :, :, :-1] - image[ :, :, 1:]), 1, keepdim=True)\n",
    "            image_dy = torch.mean(torch.abs(image[ :, :-1, :] - image[ :, 1:, :]), 1, keepdim=True)\n",
    "            weight_x = torch.exp(-image_dx)\n",
    "            weight_y = torch.exp(-image_dy)\n",
    "            pred_dx *= weight_x\n",
    "            pred_dy *= weight_y\n",
    "\n",
    "        return (pred_dx.mean() + pred_dy.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5414e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "# get current time for logging\n",
    "from datetime import datetime\n",
    "log_path = os.path.join(LOG_DIR, datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + '_train.log')\n",
    "writer = SummaryWriter(log_dir=log_path, comment='No Validation Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a8a61b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1149"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8359ecaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LuCo\\Documents\\repos\\DeepAnything\\DepthAnything\\data\\train\\train\n",
      "23971 21574 2397\n",
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]: 100%|██████████| 2697/2697 [4:16:47<00:00,  5.71s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Validation]: 100%|██████████| 300/300 [07:36<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3576\n",
      "Patience counter: 1/5\n",
      "Epoch 2/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]: 100%|██████████| 2697/2697 [4:20:46<00:00,  5.80s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Validation]: 100%|██████████| 300/300 [07:40<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3582\n",
      "Patience counter: 2/5\n",
      "Epoch 3/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]: 100%|██████████| 2697/2697 [4:20:41<00:00,  5.80s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Validation]: 100%|██████████| 300/300 [07:39<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3575\n",
      "Patience counter: 3/5\n",
      "Epoch 4/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]:   1%|          | 26/2697 [02:27<4:13:01,  5.68s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 178\u001b[39m\n\u001b[32m    171\u001b[39m             torch.save(model.state_dict(), PATIENCE_CHECKPOINT_PATH)\n\u001b[32m    172\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     50\u001b[39m model.train()\n\u001b[32m     51\u001b[39m train_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m [Train]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrgb\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LuCo\\miniforge3\\envs\\CIL\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LuCo\\miniforge3\\envs\\CIL\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:735\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    732\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    733\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    734\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    737\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    738\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    740\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    741\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LuCo\\miniforge3\\envs\\CIL\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:791\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    790\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m791\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    792\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    793\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LuCo\\miniforge3\\envs\\CIL\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LuCo\\miniforge3\\envs\\CIL\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LuCo\\miniforge3\\envs\\CIL\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:416\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mDepthDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Load depth map\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     depth = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdepth_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Apply data augmentation (e.g. horizontal flip)\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LuCo\\miniforge3\\envs\\CIL\\Lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    425\u001b[39m     own_fid = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m     fid = stack.enter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    428\u001b[39m     own_fid = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    dataset = DepthDataset(DATA_DIR, mode='train')\n",
    "    val_size = int(len(dataset) * 0.1)\n",
    "    train_size = len(dataset) - val_size\n",
    "\n",
    "    print(dataset.root)\n",
    "    print(len(dataset), train_size, val_size)\n",
    "    train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=8, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_set, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "    model_configs = {\n",
    "    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]\n",
    "             },\n",
    "    'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}\n",
    "    }\n",
    "\n",
    "    # Load the model with the specified encoder\n",
    "    model = DepthAnythingV2(**model_configs[ENCODER])\n",
    "\n",
    "    for param in model.pretrained.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "\n",
    "        \n",
    "    for param in model.depth_head.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "    # Load the pretrained weights\n",
    "    model.pretrained.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, 'epoch_depth_anything_v2_checkpoint.pth')), strict=False)\n",
    "    model = model.to(DEVICE)\n",
    "    loss_fn_si_RMSE = ScaleInvariantRMSELoss()\n",
    "    #loss_fn_gradient = DepthGradientLoss(edge_aware=True)\n",
    "\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {\"params\" : model.pretrained.parameters(), \"lr\" : 1e-5},\n",
    "        {\"params\" : model.depth_head.parameters(), \"lr\" : 1e-5}], weight_decay=1e-4)\n",
    "\n",
    "    best_val_loss = 0.22\n",
    "\n",
    "\n",
    "    \n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for i, sample in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\")):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            rgb = sample['image']\n",
    "            depth = sample['depth']\n",
    "\n",
    "            rgb = rgb.to(DEVICE)\n",
    "            depth = depth.to(DEVICE)\n",
    "\n",
    "            # print(f\"Model device: {next(model.parameters()).device}\")\n",
    "            # print(f\"RGB device: {rgb.device}, Depth device: {depth.device}\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(rgb)\n",
    "\n",
    "            #loss_fn_gradient_ = loss_fn_gradient(pred, depth)\n",
    "            loss_fn_si_RMSE_ = loss_fn_si_RMSE(pred, depth)\n",
    "            #loss_l1 = torch.mean(torch.abs(torch.log(pred + 1e-6) - torch.log(depth + 1e-6)))\n",
    "\n",
    "            loss = loss_fn_si_RMSE_ # + 0.01 * loss_l1 + 0.25 * loss_fn_gradient_\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # logging the loss\n",
    "            if i % 50 == 0:\n",
    "                #writer.add_scalar('Loss/train', loss.detach().item(), epoch * len(train_loader) + i)\n",
    "                writer.add_scalar('Loss/train_si_RMSE', loss_fn_si_RMSE_.detach().item(), epoch * len(train_loader) + i)\n",
    "                #writer.add_scalar('Loss/train_gradient', loss_fn_gradient_.item(), epoch * len(train_loader) + i)\n",
    "                #writer.add_scalar('Loss/train_l1', loss_l1.item(), epoch * len(train_loader) + i)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        writer.add_scalar('Loss/Train_Epoch', train_loss, epoch)\n",
    "\n",
    "        #Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for sample in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Validation]\"):\n",
    "                rgb = sample['image']\n",
    "                depth = sample['depth']\n",
    "                rgb = rgb.to(DEVICE)\n",
    "                depth = depth.to(DEVICE)\n",
    "\n",
    "                pred = model(rgb)\n",
    "                val_loss_ = loss_fn_si_RMSE(pred, depth)\n",
    "                val_loss += val_loss_.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        writer.add_scalar('Loss/Validation_Epoch', val_loss, epoch)\n",
    "        \n",
    "        # Save the model if validation loss improves\n",
    "        if not os.path.exists(CHECKPOINT_DIR):\n",
    "            os.makedirs(CHECKPOINT_DIR)\n",
    "        \n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Move the previous best checkpoint to a backup location\n",
    "            if os.path.exists(BEST_CHECKPOINT_PATH):\n",
    "                shutil.move(BEST_CHECKPOINT_PATH, BACKUP_CHECKPOINT_PATH)\n",
    "\n",
    "            torch.save(model.state_dict(), BEST_CHECKPOINT_PATH)\n",
    "            print(f\"Model saved to {BEST_CHECKPOINT_PATH}\")\n",
    "            print(f\"Previous best model moved to {BACKUP_CHECKPOINT_PATH}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Patience counter: {patience_counter}/{patience}\")\n",
    "            torch.save(model.state_dict(), PATIENCE_CHECKPOINT_PATH)\n",
    "        \n",
    "        \n",
    "        # Save the model every epoch\n",
    "        torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, f\"epoch_depth_anything_v2_checkpoint.pth\"))\n",
    "\n",
    "        # Save a sample image and depth map for visualization\n",
    "\n",
    "        # Load a sample from the training set\n",
    "        rgb = cv2.imread(os.path.join(DATA_DIR, \"sample_000000_rgb.png\"))\n",
    "        gt = np.load(os.path.join(DATA_DIR, \"sample_000000_depth.npy\"))\n",
    "\n",
    "        # set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            pred = model.infer_image(rgb)\n",
    "        d_min = np.min(pred)\n",
    "        d_max = np.max(pred)\n",
    "\n",
    "        depth_vis = (pred - d_min) / (d_max - d_min + 1e-6)\n",
    "        \n",
    "        cmap = matplotlib.colormaps.get_cmap('plasma')\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title('Input Image')\n",
    "        plt.axis('off')\n",
    "        plt.imshow(rgb)\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title('Predicted Depth Map')\n",
    "        plt.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "        plt.imshow(depth_vis, cmap=cmap)\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title('Ground Truth')\n",
    "        plt.axis('off')\n",
    "        plt.imshow(gt, cmap=cmap)\n",
    "        plt.savefig(os.path.join(RESULT_DIR, f\"epoch_{epoch+1}_sample.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            torch.save(model.state_dict(), PATIENCE_CHECKPOINT_PATH)\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b127a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Test Predictions: 100%|██████████| 130/130 [02:07<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_test_predictions(model, test_loader, device):\n",
    "    \"\"\"Generate predictions for the test set without ground truth\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    predictions_dir = os.path.join(RESULT_DIR, 'test_predictions')\n",
    "    # Ensure predictions directory exists\n",
    "    if not os.path.exists(predictions_dir):\n",
    "        os.makedirs(predictions_dir)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sample in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n",
    "            inputs = sample['image']\n",
    "            filenames = sample['filenames']\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            batch_size = inputs.size(0)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Ensure outputs have the correct shape before resizing\n",
    "            if outputs.dim() == 3:  # If outputs have shape (N, H, W)\n",
    "                outputs = outputs.unsqueeze(1)  # Add a channel dimension to make it (N, C, H, W)\n",
    "\n",
    "            # Resize outputs to match original input dimensions (426x560)\n",
    "            outputs = nn.functional.interpolate(\n",
    "                outputs,\n",
    "                size=(426, 560),  # Original input dimensions\n",
    "                mode='bilinear',\n",
    "                align_corners=True\n",
    "            )\n",
    "            \n",
    "            # Save all test predictions\n",
    "            for i in range(batch_size):\n",
    "                # Get filename without extension\n",
    "                filename = filenames[i].split('\\\\')[-1].replace('_rgb.png', '_depth.npy')\n",
    "                \n",
    "                # Save depth map prediction as numpy array\n",
    "                depth_pred = outputs[i].cpu().squeeze().numpy()\n",
    "                np.save(os.path.join(predictions_dir, f\"{filename}\"), depth_pred)\n",
    "            \n",
    "            # Clean up memory\n",
    "            del inputs, outputs\n",
    "        \n",
    "        # Clear cache after test predictions\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "TESTDIR = os.path.join(os.getcwd(), 'data', \"test\", \"test\")\n",
    "test_dataset = DepthDataset(TESTDIR, mode='test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=5, shuffle=False, num_workers=0)\n",
    "\n",
    "model_configs = {\n",
    "    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
    "    'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}\n",
    "}\n",
    "\n",
    "model = DepthAnythingV2(**model_configs[ENCODER])\n",
    "model.load_state_dict(torch.load(os.path.join(BEST_CHECKPOINT_PATH), map_location=DEVICE), strict=False)\n",
    "model = model.to(DEVICE)\n",
    "generate_test_predictions(model, test_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b7489d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LuCo\\Documents\\repos\\DeepAnything\\DepthAnything\n",
      "c:\\Users\\LuCo\\Documents\\repos\\DeepAnything\\DepthAnything\\results\\test_predictions\n",
      "c:\\Users\\LuCo\\Documents\\repos\\DeepAnything\\DepthAnything\\data\\test_list.txt\n",
      "c:\\Users\\LuCo\\Documents\\repos\\DeepAnything\\DepthAnything\\results\\csv\\FT_DepthAnything_Best.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing depth maps: 100%|██████████| 650/650 [01:46<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved to: c:\\Users\\LuCo\\Documents\\repos\\DeepAnything\\DepthAnything\\results\\csv\\FT_DepthAnything_Best.csv\n",
      "Shape of the CSV: (650, 2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), 'src'))\n",
    "from src.create_prediction_csv import get_submission\n",
    "\n",
    "get_submission()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CIL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
