{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b29d95d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "xFormers not available\n",
      "xFormers not available\n",
      "xFormers not available\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2 as cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip, \\\n",
    "                RandomVerticalFlip, ColorJitter, RandomRotation, GaussianBlur, RandomCrop, RandomPerspective, RandomAffine\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "sys.path.append('C:\\\\Users\\\\LuCo\\\\Documents\\\\repos\\\\Depth-Anything-V2')\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "from metric_depth.dataset.transform import Resize, NormalizeImage, PrepareForNet, Crop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fda8a4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Parameters\n",
    "# ------------------------------\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DATA_DIR = os.path.join(os.getcwd(), 'data', \"train\", \"train\")\n",
    "CHECKPOINT_DIR = os.path.join(os.getcwd(), 'checkpoints')\n",
    "BEST_CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, 'best_depth_anything_v2.pth')\n",
    "BACKUP_CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, 'depth_anything_v2.pth')\n",
    "PATIENCE_CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, 'patience_depth_anything_v2.pth')\n",
    "ENCODER = 'vitl'\n",
    "NUM_EPOCHS = 10\n",
    "LOG_DIR = os.path.join(os.getcwd(), 'logs')\n",
    "RESULT_DIR = os.path.join(os.getcwd(), 'results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffe81878",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthDataset(Dataset):\n",
    "    def __init__(self, root, mode='train', size = (518, 518)):\n",
    "        self.root = root\n",
    "        self.mode = mode\n",
    "        self.size = size\n",
    "\n",
    "        self.rgb_paths = sorted(glob.glob(os.path.join(root, '*_rgb.png')))\n",
    "        self.depth_paths = [p.replace('_rgb.png', '_depth.npy') for p in self.rgb_paths]\n",
    "\n",
    "        net_w, net_h = size\n",
    "\n",
    "\n",
    "        self.transform = Compose([\n",
    "            Resize(width=net_w, height=net_h, \n",
    "                   resize_target=True if mode == 'train' else False, \n",
    "                   keep_aspect_ratio=True, \n",
    "                   ensure_multiple_of=14, \n",
    "                   resize_method=\"lower_bound\",\n",
    "                   image_interpolation_method=cv2.INTER_CUBIC),\n",
    "            NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            PrepareForNet(),\n",
    "        ] + ([Crop(size[0])] if mode == 'train' else []))\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_paths)\n",
    "    \n",
    "\n",
    "    def horizontal_flip(self, image, depth, p=0.3):\n",
    "        if np.random.rand() < p:\n",
    "            image = np.fliplr(image).copy()\n",
    "            depth = np.fliplr(depth).copy()\n",
    "        return image, depth\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load RGB image\n",
    "        rgb = cv2.imread(self.rgb_paths[idx])\n",
    "        rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB) / 255.0\n",
    "\n",
    "        # Load depth map\n",
    "        depth = np.load(self.depth_paths[idx])\n",
    "\n",
    "        # Apply data augmentation (e.g. horizontal flip)\n",
    "        if self.mode == 'train':\n",
    "            rgb, depth = self.horizontal_flip(rgb, depth, p=0.3)\n",
    "\n",
    "        sample = self.transform({'image': rgb, 'depth': depth})\n",
    "\n",
    "        sample['image'] = torch.from_numpy(sample['image'])\n",
    "        sample['depth'] = torch.from_numpy(sample['depth'])\n",
    "\n",
    "        sample['valid_mask'] = (torch.isnan(sample['depth']) == 0)\n",
    "        sample['depth'][sample['valid_mask'] == 0] = 0\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f7e2099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleInvariantRMSELoss(nn.Module):\n",
    "    def forward(self, pred, target):\n",
    "        pred = torch.clamp(pred, min=1e-6)\n",
    "        target = torch.clamp(target, min=1e-6)\n",
    "        diff = torch.log(pred) - torch.log(target)\n",
    "        alpha = torch.mean(diff)\n",
    "        return torch.sqrt(torch.mean((diff - alpha)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe4059a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthGradientLoss(nn.Module):\n",
    "    def __init__(self, edge_aware=True):\n",
    "        super(DepthGradientLoss, self).__init__()\n",
    "        self.edge_aware = edge_aware\n",
    "\n",
    "    def forward(self, pred, image):\n",
    "        pred_dx = torch.abs(pred[ :, :, :-1] - pred[ :, :, 1:])\n",
    "        pred_dy = torch.abs(pred[ :, :-1, :] - pred[ :, 1:, :])\n",
    "\n",
    "        if self.edge_aware:\n",
    "            image_dx = torch.mean(torch.abs(image[ :, :, :-1] - image[ :, :, 1:]), 1, keepdim=True)\n",
    "            image_dy = torch.mean(torch.abs(image[ :, :-1, :] - image[ :, 1:, :]), 1, keepdim=True)\n",
    "            weight_x = torch.exp(-image_dx)\n",
    "            weight_y = torch.exp(-image_dy)\n",
    "            pred_dx *= weight_x\n",
    "            pred_dy *= weight_y\n",
    "\n",
    "        return (pred_dx.mean() + pred_dy.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5414e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "# get current time for logging\n",
    "from datetime import datetime\n",
    "log_path = os.path.join(LOG_DIR, datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + '_train.log')\n",
    "writer = SummaryWriter(log_dir=log_path, comment='Data_Augmentation_experiment_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8359ecaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LuCo\\Documents\\repos\\DeepAnything\\DepthAnything\\data\\train\\train\n",
      "23971 21574 2397\n",
      "Epoch 1/10\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:   0%|          | 0/4315 [00:00<?, ?it/s]C:\\Users\\LuCo\\AppData\\Local\\Temp\\ipykernel_33304\\1158987737.py:75: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\Scalar.cpp:23.)\n",
      "  writer.add_scalar('Loss/train', loss.item(), epoch * len(train_loader) + i)\n",
      "Epoch 1 [Train]:   0%|          | 1/4315 [00:01<1:25:20,  1.19s/it]C:\\Users\\LuCo\\AppData\\Local\\Temp\\ipykernel_33304\\1158987737.py:75: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\Scalar.cpp:23.)\n",
      "  writer.add_scalar('Loss/train', loss.item(), epoch * len(train_loader) + i)\n",
      "Epoch 1 [Train]: 100%|██████████| 4315/4315 [1:03:51<00:00,  1.13it/s]\n",
      "Epoch 1 [Train]: 100%|██████████| 4315/4315 [1:03:51<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Validation]: 100%|██████████| 480/480 [05:58<00:00,  1.34it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1372\n",
      "Patience counter: 1/5\n",
      "Epoch 2/10\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]:  23%|██▎       | 988/4315 [14:20<50:51,  1.09it/s]  "
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    dataset = DepthDataset(DATA_DIR, mode='train')\n",
    "    val_size = int(len(dataset) * 0.1)\n",
    "    train_size = len(dataset) - val_size\n",
    "\n",
    "    print(dataset.root)\n",
    "    print(len(dataset), train_size, val_size)\n",
    "    train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=5, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_set, batch_size=5, shuffle=False, num_workers=0)\n",
    "\n",
    "    model_configs = {\n",
    "    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
    "    'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}\n",
    "    }\n",
    "\n",
    "    # Load the model with the specified encoder\n",
    "    model = DepthAnythingV2(**model_configs[ENCODER])\n",
    "\n",
    "    for param in model.pretrained.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in model.depth_head.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "    model.load_state_dict(torch.load(BEST_CHECKPOINT_PATH, map_location=DEVICE), strict=False)\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    loss_fn_si_RMSE = ScaleInvariantRMSELoss()\n",
    "    loss_fn_gradient = DepthGradientLoss(edge_aware=True)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-6, weight_decay=1e-4)\n",
    "\n",
    "    best_val_loss = 0.10\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for i, sample in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\")):\n",
    "\n",
    "            \n",
    "            rgb = sample['image']\n",
    "            depth = sample['depth']\n",
    "\n",
    "            rgb = rgb.to(DEVICE)\n",
    "            depth = depth.to(DEVICE)\n",
    "\n",
    "            # print(f\"Model device: {next(model.parameters()).device}\")\n",
    "            # print(f\"RGB device: {rgb.device}, Depth device: {depth.device}\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(rgb)\n",
    "\n",
    "            loss_fn_gradient_ = loss_fn_gradient(pred, depth)\n",
    "            loss_fn_si_RMSE_ = loss_fn_si_RMSE(pred, depth)\n",
    "            loss = loss_fn_si_RMSE_ + loss_fn_gradient_\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # logging the loss\n",
    "            if i % 50 == 0:\n",
    "                writer.add_scalar('Loss/train', loss.item(), epoch * len(train_loader) + i)\n",
    "                writer.add_scalar('Loss/train_si_RMSE', loss_fn_si_RMSE_.item(), epoch * len(train_loader) + i)\n",
    "                writer.add_scalar('Loss/train_gradient', loss_fn_gradient_.item(), epoch * len(train_loader) + i)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        writer.add_scalar('Loss/Train_Epoch', train_loss, epoch)\n",
    "\n",
    "        #Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for sample in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Validation]\"):\n",
    "                rgb = sample['image']\n",
    "                depth = sample['depth']\n",
    "                rgb = rgb.to(DEVICE)\n",
    "                depth = depth.to(DEVICE)\n",
    "\n",
    "                pred = model(rgb)\n",
    "                val_loss_ = loss_fn_si_RMSE(pred, depth)\n",
    "                val_loss += val_loss_.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        writer.add_scalar('Loss/Validation_Epoch', val_loss, epoch)\n",
    "\n",
    "        # Save the model if validation loss improves\n",
    "        if not os.path.exists(CHECKPOINT_DIR):\n",
    "            os.makedirs(CHECKPOINT_DIR)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Move the previous best checkpoint to a backup location\n",
    "            if os.path.exists(BEST_CHECKPOINT_PATH):\n",
    "                shutil.move(BEST_CHECKPOINT_PATH, BACKUP_CHECKPOINT_PATH)\n",
    "\n",
    "            torch.save(model.state_dict(), BEST_CHECKPOINT_PATH)\n",
    "            print(f\"Model saved to {BEST_CHECKPOINT_PATH}\")\n",
    "            print(f\"Previous best model moved to {BACKUP_CHECKPOINT_PATH}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Patience counter: {patience_counter}/{patience}\")\n",
    "        \n",
    "        # Save a sample image and depth map for visualization\n",
    "\n",
    "        # Load a sample from the training set\n",
    "        rgb = cv2.imread(os.path.join(DATA_DIR, \"sample_000000_rgb.png\"))\n",
    "        gt = np.load(os.path.join(DATA_DIR, \"sample_000000_depth.npy\"))\n",
    "\n",
    "        # set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            pred = model.infer_image(rgb)\n",
    "        d_min = np.min(pred)\n",
    "        d_max = np.max(pred)\n",
    "\n",
    "        depth_vis = (pred - d_min) / (d_max - d_min + 1e-6)\n",
    "        \n",
    "        cmap = matplotlib.colormaps.get_cmap('plasma')\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title('Input Image')\n",
    "        plt.axis('off')\n",
    "        plt.imshow(rgb)\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title('Predicted Depth Map')\n",
    "        plt.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "        plt.imshow(depth_vis, cmap=cmap)\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title('Ground Truth')\n",
    "        plt.axis('off')\n",
    "        plt.imshow(gt, cmap=cmap)\n",
    "        plt.savefig(os.path.join(RESULT_DIR, f\"epoch_{epoch+1}_sample.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            torch.save(model.state_dict(), PATIENCE_CHECKPOINT_PATH)\n",
    "            break\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CIL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
