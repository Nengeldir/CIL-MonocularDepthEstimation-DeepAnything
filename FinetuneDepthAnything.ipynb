{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b29d95d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2 as cv2\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip, \\\n",
    "                RandomVerticalFlip, ColorJitter, RandomRotation, GaussianBlur, RandomCrop, RandomPerspective, RandomAffine\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "sys.path.append('C:\\\\Users\\\\LuCo\\\\Documents\\\\repos\\\\Depth-Anything-V2')\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "from metric_depth.dataset.transform import Resize, NormalizeImage, PrepareForNet, Crop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fda8a4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Parameters\n",
    "# ------------------------------\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DATA_DIR = os.path.join(os.getcwd(), 'data', \"train\", \"train\")\n",
    "CHECKPOINT_DIR = os.path.join(os.getcwd(), 'checkpoints')\n",
    "BEST_CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, 'best_depth_anything_v2.pth')\n",
    "BACKUP_CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, 'depth_anything_v2.pth')\n",
    "PATIENCE_CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, 'patience_depth_anything_v2_vitg.pth')\n",
    "ENCODER = 'vitl'\n",
    "NUM_EPOCHS = 4\n",
    "LOG_DIR = os.path.join(os.getcwd(), 'logs')\n",
    "RESULT_DIR = os.path.join(os.getcwd(), 'results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ffe81878",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthDataset(Dataset):\n",
    "    def __init__(self, root, mode='train', size = (518, 518)):\n",
    "        self.root = root\n",
    "        self.mode = mode\n",
    "        self.size = size\n",
    "\n",
    "        self.rgb_paths = sorted(glob.glob(os.path.join(root, '*_rgb.png')))\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            self.depth_paths = [p.replace('_rgb.png', '_depth.npy') for p in self.rgb_paths]\n",
    "\n",
    "        net_w, net_h = size\n",
    "\n",
    "\n",
    "        self.transform = Compose([\n",
    "            Resize(width=net_w, height=net_h, \n",
    "                   resize_target=True if mode == 'train' else False, \n",
    "                   keep_aspect_ratio=True, \n",
    "                   ensure_multiple_of=14, \n",
    "                   resize_method=\"lower_bound\",\n",
    "                   image_interpolation_method=cv2.INTER_CUBIC),\n",
    "            NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            PrepareForNet(),\n",
    "        ] + ([Crop(size[0])] if mode == 'train' else []))\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_paths)\n",
    "    \n",
    "\n",
    "    def horizontal_flip(self, image, depth, p=0.3):\n",
    "        if np.random.rand() < p:\n",
    "            image = np.fliplr(image).copy()\n",
    "            depth = np.fliplr(depth).copy()\n",
    "        return image, depth\n",
    "    \n",
    "    def brightness_jitter(self, image, p = 0.25):\n",
    "        if np.random.rand() < p:\n",
    "            hsv = cv2.cvtColor((image * 255).astype(np.uint8), cv2.COLOR_RGB2HSV)\n",
    "            hsv = np.array(hsv, dtype=np.float32)\n",
    "            hsv[..., 2] *= 0.8 + 0.4 * np.random.rand()\n",
    "            hsv[..., 2] = np.clip(hsv[..., 2], 0, 255)\n",
    "            rgb = cv2.cvtColor(hsv.astype(np.uint8), cv2.COLOR_HSV2RGB)\n",
    "            image = rgb.astype(np.float32) / 255.0\n",
    "            \n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load RGB image\n",
    "        rgb = cv2.imread(self.rgb_paths[idx])\n",
    "        rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB) / 255.0\n",
    "        \n",
    "        # Load depth map\n",
    "        if self.mode == 'train':\n",
    "            depth = np.load(self.depth_paths[idx])\n",
    "\n",
    "        # Apply data augmentation (e.g. horizontal flip)\n",
    "        if self.mode == 'train':\n",
    "            rgb, depth = self.horizontal_flip(rgb, depth, p=0.1)\n",
    "            rgb = self.brightness_jitter(rgb, p=0.1)\n",
    "\n",
    "            sample = self.transform({'image': rgb, 'depth': depth})\n",
    "        \n",
    "        if self.mode == 'test':\n",
    "            sample = self.transform({'image': rgb})\n",
    "            sample['image'] = torch.from_numpy(sample['image'])\n",
    "\n",
    "        if self.mode == 'test':\n",
    "            sample['filenames'] = self.rgb_paths[idx]\n",
    "            \n",
    "        if self.mode == 'train':\n",
    "            sample['depth'] = torch.from_numpy(sample['depth'])\n",
    "\n",
    "            sample['valid_mask'] = (torch.isnan(sample['depth']) == 0)\n",
    "            sample['depth'][sample['valid_mask'] == 0] = 0\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f7e2099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleInvariantRMSELoss(nn.Module):\n",
    "    def forward(self, pred, target):\n",
    "        pred = torch.clamp(pred, min=1e-6)\n",
    "        target = torch.clamp(target, min=1e-6)\n",
    "        diff = torch.log(pred) - torch.log(target)\n",
    "        alpha = torch.mean(diff)\n",
    "        return torch.sqrt(torch.mean((diff - alpha)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe4059a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthGradientLoss(nn.Module):\n",
    "    def __init__(self, edge_aware=True):\n",
    "        super(DepthGradientLoss, self).__init__()\n",
    "        self.edge_aware = edge_aware\n",
    "\n",
    "    def forward(self, pred, image):\n",
    "        pred_dx = torch.abs(pred[ :, :, :-1] - pred[ :, :, 1:])\n",
    "        pred_dy = torch.abs(pred[ :, :-1, :] - pred[ :, 1:, :])\n",
    "\n",
    "        if self.edge_aware:\n",
    "            image_dx = torch.mean(torch.abs(image[ :, :, :-1] - image[ :, :, 1:]), 1, keepdim=True)\n",
    "            image_dy = torch.mean(torch.abs(image[ :, :-1, :] - image[ :, 1:, :]), 1, keepdim=True)\n",
    "            weight_x = torch.exp(-image_dx)\n",
    "            weight_y = torch.exp(-image_dy)\n",
    "            pred_dx *= weight_x\n",
    "            pred_dy *= weight_y\n",
    "\n",
    "        return (pred_dx.mean() + pred_dy.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5414e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "# get current time for logging\n",
    "from datetime import datetime\n",
    "log_path = os.path.join(LOG_DIR, datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + '_train.log')\n",
    "writer = SummaryWriter(log_dir=log_path, comment='No Validation Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a8a61b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8898"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8359ecaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LuCo\\Documents\\repos\\DeepAnything\\DepthAnything\\data\\train\\train\n",
      "23971 23971 0\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]: 100%|██████████| 2997/2997 [1:43:25<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1326\n",
      "Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]: 100%|██████████| 2997/2997 [1:45:39<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1314\n",
      "Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]: 100%|██████████| 2997/2997 [1:45:36<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1297\n",
      "Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]: 100%|██████████| 2997/2997 [1:45:13<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1293\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    dataset = DepthDataset(DATA_DIR, mode='train')\n",
    "    val_size = int(len(dataset) * 0.0)\n",
    "    train_size = len(dataset) - val_size\n",
    "\n",
    "    print(dataset.root)\n",
    "    print(len(dataset), train_size, val_size)\n",
    "    train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=8, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_set, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "    model_configs = {\n",
    "    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
    "    'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}\n",
    "    }\n",
    "\n",
    "    # Load the model with the specified encoder\n",
    "    model = DepthAnythingV2(**model_configs[ENCODER])\n",
    "\n",
    "    for param in model.pretrained.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "        \n",
    "    for param in model.depth_head.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "    #checkpoint = torch.load(BEST_CHECKPOINT_PATH, map_location=DEVICE)\n",
    "\n",
    "    \n",
    "    model.load_state_dict(torch.load(BEST_CHECKPOINT_PATH, map_location=DEVICE), strict=False)\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    loss_fn_si_RMSE = ScaleInvariantRMSELoss()\n",
    "    #loss_fn_gradient = DepthGradientLoss(edge_aware=True)\n",
    "\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {\"params\" : model.pretrained.parameters(), \"lr\" : 1e-5},\n",
    "        {\"params\" : model.depth_head.parameters(), \"lr\" : 5e-6}], weight_decay=1e-4)\n",
    "\n",
    "    best_val_loss = 0.105\n",
    "\n",
    "    \n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for i, sample in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\")):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            rgb = sample['image']\n",
    "            depth = sample['depth']\n",
    "\n",
    "            rgb = rgb.to(DEVICE)\n",
    "            depth = depth.to(DEVICE)\n",
    "\n",
    "            # print(f\"Model device: {next(model.parameters()).device}\")\n",
    "            # print(f\"RGB device: {rgb.device}, Depth device: {depth.device}\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(rgb)\n",
    "\n",
    "            #loss_fn_gradient_ = loss_fn_gradient(pred, depth)\n",
    "            loss_fn_si_RMSE_ = loss_fn_si_RMSE(pred, depth)\n",
    "            #loss_l1 = torch.mean(torch.abs(torch.log(pred + 1e-6) - torch.log(depth + 1e-6)))\n",
    "\n",
    "            loss = loss_fn_si_RMSE_ # + 0.01 * loss_l1 + 0.25 * loss_fn_gradient_\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # logging the loss\n",
    "            if i % 50 == 0:\n",
    "                #writer.add_scalar('Loss/train', loss.detach().item(), epoch * len(train_loader) + i)\n",
    "                writer.add_scalar('Loss/train_si_RMSE', loss_fn_si_RMSE_.detach().item(), epoch * len(train_loader) + i)\n",
    "                #writer.add_scalar('Loss/train_gradient', loss_fn_gradient_.item(), epoch * len(train_loader) + i)\n",
    "                #writer.add_scalar('Loss/train_l1', loss_l1.item(), epoch * len(train_loader) + i)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        writer.add_scalar('Loss/Train_Epoch', train_loss, epoch)\n",
    "\n",
    "        #Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            for sample in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Validation]\"):\n",
    "                rgb = sample['image']\n",
    "                depth = sample['depth']\n",
    "                rgb = rgb.to(DEVICE)\n",
    "                depth = depth.to(DEVICE)\n",
    "\n",
    "                pred = model(rgb)\n",
    "                val_loss_ = loss_fn_si_RMSE(pred, depth)\n",
    "                val_loss += val_loss_.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        writer.add_scalar('Loss/Validation_Epoch', val_loss, epoch)\n",
    "        \"\"\"\n",
    "        # Save the model if validation loss improves\n",
    "        if not os.path.exists(CHECKPOINT_DIR):\n",
    "            os.makedirs(CHECKPOINT_DIR)\n",
    "        \n",
    "        \"\"\"\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Move the previous best checkpoint to a backup location\n",
    "            if os.path.exists(BEST_CHECKPOINT_PATH):\n",
    "                shutil.move(BEST_CHECKPOINT_PATH, BACKUP_CHECKPOINT_PATH)\n",
    "\n",
    "            torch.save(model.state_dict(), BEST_CHECKPOINT_PATH)\n",
    "            print(f\"Model saved to {BEST_CHECKPOINT_PATH}\")\n",
    "            print(f\"Previous best model moved to {BACKUP_CHECKPOINT_PATH}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Patience counter: {patience_counter}/{patience}\")\n",
    "            torch.save(model.state_dict(), PATIENCE_CHECKPOINT_PATH)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Save the model every epoch\n",
    "        torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, f\"epoch_depth_anything_v2_full.pth\"))\n",
    "\n",
    "        # Save a sample image and depth map for visualization\n",
    "\n",
    "        # Load a sample from the training set\n",
    "        rgb = cv2.imread(os.path.join(DATA_DIR, \"sample_000000_rgb.png\"))\n",
    "        gt = np.load(os.path.join(DATA_DIR, \"sample_000000_depth.npy\"))\n",
    "\n",
    "        # set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            pred = model.infer_image(rgb)\n",
    "        d_min = np.min(pred)\n",
    "        d_max = np.max(pred)\n",
    "\n",
    "        depth_vis = (pred - d_min) / (d_max - d_min + 1e-6)\n",
    "        \n",
    "        cmap = matplotlib.colormaps.get_cmap('plasma')\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title('Input Image')\n",
    "        plt.axis('off')\n",
    "        plt.imshow(rgb)\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title('Predicted Depth Map')\n",
    "        plt.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "        plt.imshow(depth_vis, cmap=cmap)\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title('Ground Truth')\n",
    "        plt.axis('off')\n",
    "        plt.imshow(gt, cmap=cmap)\n",
    "        plt.savefig(os.path.join(RESULT_DIR, f\"epoch_{epoch+1}_sample.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            torch.save(model.state_dict(), PATIENCE_CHECKPOINT_PATH)\n",
    "            break\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b127a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Test Predictions:  53%|█████▎    | 69/130 [01:06<00:59,  1.02it/s]"
     ]
    }
   ],
   "source": [
    "def generate_test_predictions(model, test_loader, device):\n",
    "    \"\"\"Generate predictions for the test set without ground truth\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    predictions_dir = os.path.join(RESULT_DIR, 'test_predictions')\n",
    "    # Ensure predictions directory exists\n",
    "    if not os.path.exists(predictions_dir):\n",
    "        os.makedirs(predictions_dir)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sample in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n",
    "            inputs = sample['image']\n",
    "            filenames = sample['filenames']\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            batch_size = inputs.size(0)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Ensure outputs have the correct shape before resizing\n",
    "            if outputs.dim() == 3:  # If outputs have shape (N, H, W)\n",
    "                outputs = outputs.unsqueeze(1)  # Add a channel dimension to make it (N, C, H, W)\n",
    "\n",
    "            # Resize outputs to match original input dimensions (426x560)\n",
    "            outputs = nn.functional.interpolate(\n",
    "                outputs,\n",
    "                size=(426, 560),  # Original input dimensions\n",
    "                mode='bilinear',\n",
    "                align_corners=True\n",
    "            )\n",
    "            \n",
    "            # Save all test predictions\n",
    "            for i in range(batch_size):\n",
    "                # Get filename without extension\n",
    "                filename = filenames[i].split('\\\\')[-1].replace('_rgb.png', '_depth.npy')\n",
    "                \n",
    "                # Save depth map prediction as numpy array\n",
    "                depth_pred = outputs[i].cpu().squeeze().numpy()\n",
    "                np.save(os.path.join(predictions_dir, f\"{filename}\"), depth_pred)\n",
    "            \n",
    "            # Clean up memory\n",
    "            del inputs, outputs\n",
    "        \n",
    "        # Clear cache after test predictions\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "TESTDIR = os.path.join(os.getcwd(), 'data', \"test\", \"test\")\n",
    "test_dataset = DepthDataset(TESTDIR, mode='test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=5, shuffle=False, num_workers=0)\n",
    "\n",
    "model_configs = {\n",
    "    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
    "    'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}\n",
    "}\n",
    "\n",
    "model = DepthAnythingV2(**model_configs[ENCODER])\n",
    "model.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, \"epoch_depth_anything_v2_full.pth\"), map_location=DEVICE), strict=False)\n",
    "model = model.to(DEVICE)\n",
    "generate_test_predictions(model, test_loader, DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CIL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
