{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b29d95d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2 as cv2\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "sys.path.append('C:\\\\Users\\\\LuCo\\\\Documents\\\\repos\\\\Depth-Anything-V2')\n",
    "from depth_anything_v2.dpt import DepthAnythingV2\n",
    "from metric_depth.dataset.transform import Resize, NormalizeImage, PrepareForNet, Crop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fda8a4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Parameters\n",
    "# ------------------------------\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DATA_DIR = os.path.join(os.getcwd(), 'data', \"train\", \"train\")\n",
    "CHECKPOINT_DIR = os.path.join(os.getcwd(), 'checkpoints')\n",
    "BEST_CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, 'best_depth_anything_v2.pth')\n",
    "BACKUP_CHECKPOINT_PATH = os.path.join(CHECKPOINT_DIR, 'depth_anything_v2.pth')\n",
    "ENCODER = 'vitl'\n",
    "NUM_EPOCHS = 10\n",
    "LOG_DIR = os.path.join(os.getcwd(), 'logs')\n",
    "RESULT_DIR = os.path.join(os.getcwd(), 'results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe81878",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthDataset(Dataset):\n",
    "    def __init__(self, root, mode='train', size = (518, 518), stack_size = 10):\n",
    "        self.root = root\n",
    "        self.mode = mode\n",
    "        self.size = size\n",
    "        self.stack_size = stack_size\n",
    "\n",
    "        self.rgb_paths = sorted(glob.glob(os.path.join(root, '*_rgb.png')))\n",
    "        self.depth_paths = [p.replace('_rgb.png', '_depth.npy') for p in self.rgb_paths]\n",
    "        self.focal_stack_paths = [p.replace('_rgb.png', '_focal_stack_*.png') for p in self.rgb_paths]\n",
    "\n",
    "\n",
    "        net_w, net_h = size\n",
    "        self.transform = Compose([\n",
    "            Resize(width = net_w, height = net_h, \n",
    "                   resize_target = True if mode == 'train' else False, \n",
    "                   keep_aspect_ratio = True, \n",
    "                   ensure_multiple_of = 14, \n",
    "                   resize_method = \"lower_bound\",\n",
    "                   image_interpolation_method = cv2.INTER_CUBIC),\n",
    "            NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            PrepareForNet(),\n",
    "        ] + ([Crop(size[0])] if mode == 'train' else []))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        rgb = cv2.imread(self.rgb_paths[idx])\n",
    "        rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB) / 255.0\n",
    "\n",
    "        depth = np.load(self.depth_paths[idx])\n",
    "\n",
    "\n",
    "        # focal stack\n",
    "        sample = self.transform({'image': rgb, 'depth': depth})\n",
    "\n",
    "        sample['image'] = torch.from_numpy(sample['image'])\n",
    "        sample['depth'] = torch.from_numpy(sample['depth'])\n",
    "\n",
    "        sample['valid_mask'] = (torch.isnan(sample['depth']) == 0)\n",
    "        sample['depth'][sample['valid_mask'] == 0] = 0\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f7e2099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleInvariantRMSELoss(nn.Module):\n",
    "    def forward(self, pred, target):\n",
    "        pred = torch.clamp(pred, min=1e-6)\n",
    "        target = torch.clamp(target, min=1e-6)\n",
    "        diff = torch.log(pred) - torch.log(target)\n",
    "        alpha = torch.mean(diff)\n",
    "        return torch.sqrt(torch.mean((diff - alpha)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5414e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)\n",
    "writer = SummaryWriter(log_dir=LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19deea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "depthDataset = DepthDataset(DATA_DIR, mode='train', size=(518, 518), stack_size=10)\n",
    "print(depthDataset.focal_stack_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8359ecaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LuCo\\Documents\\repos\\DeepAnything\\DepthAnything\\data\\train\\train\n",
      "23971 21574 2397\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]: 100%|██████████| 4315/4315 [1:02:01<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Validation]: 100%|██████████| 480/480 [05:42<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1113\n",
      "Model saved to c:\\Users\\LuCo\\Documents\\repos\\DeepAnything\\DepthAnything\\checkpoints\\best_depth_anything_v2.pth\n",
      "Previous best model moved to c:\\Users\\LuCo\\Documents\\repos\\DeepAnything\\DepthAnything\\checkpoints\\depth_anything_v2.pth\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]: 100%|██████████| 4315/4315 [1:01:01<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Validation]: 100%|██████████| 480/480 [05:33<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1113\n",
      "Patience counter: 1/5\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]: 100%|██████████| 4315/4315 [1:00:41<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Validation]: 100%|██████████| 480/480 [05:34<00:00,  1.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.1112\n",
      "Model saved to c:\\Users\\LuCo\\Documents\\repos\\DeepAnything\\DepthAnything\\checkpoints\\best_depth_anything_v2.pth\n",
      "Previous best model moved to c:\\Users\\LuCo\\Documents\\repos\\DeepAnything\\DepthAnything\\checkpoints\\depth_anything_v2.pth\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]:  10%|▉         | 431/4315 [05:59<53:56,  1.20it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 146\u001b[39m\n\u001b[32m    144\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEarly stopping triggered.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    145\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m50\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     65\u001b[39m         writer.add_scalar(\u001b[33m'\u001b[39m\u001b[33mLoss/train\u001b[39m\u001b[33m'\u001b[39m, loss.item(), epoch * \u001b[38;5;28mlen\u001b[39m(train_loader) + i)\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     train_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m train_loss /= \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[32m     69\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    dataset = DepthDataset(DATA_DIR, mode='train')\n",
    "    val_size = int(len(dataset) * 0.1)\n",
    "    train_size = len(dataset) - val_size\n",
    "\n",
    "    print(dataset.root)\n",
    "    print(len(dataset), train_size, val_size)\n",
    "    train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=5, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_set, batch_size=5, shuffle=False, num_workers=0)\n",
    "\n",
    "    model_configs = {\n",
    "    'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},\n",
    "    'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},\n",
    "    'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},\n",
    "    'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}\n",
    "    }\n",
    "\n",
    "    # Load the model with the specified encoder\n",
    "    model = DepthAnythingV2(**model_configs[ENCODER])\n",
    "\n",
    "    for param in model.pretrained.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in model.depth_head.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "    model.load_state_dict(torch.load(BEST_CHECKPOINT_PATH, map_location=DEVICE), strict=False)\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    loss_fn = ScaleInvariantRMSELoss()\n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-6, weight_decay=1e-4)\n",
    "\n",
    "    best_val_loss = 0.113\n",
    "\n",
    "    \n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for i, sample in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\")):\n",
    "\n",
    "            \n",
    "            rgb = sample['image']\n",
    "            depth = sample['depth']\n",
    "\n",
    "            rgb = rgb.to(DEVICE)\n",
    "            depth = depth.to(DEVICE)\n",
    "\n",
    "            # print(f\"Model device: {next(model.parameters()).device}\")\n",
    "            # print(f\"RGB device: {rgb.device}, Depth device: {depth.device}\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(rgb)\n",
    "            loss = loss_fn(pred, depth)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # logging the loss\n",
    "            if i % 50 == 0:\n",
    "                writer.add_scalar('Loss/train', loss.item(), epoch * len(train_loader) + i)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        writer.add_scalar('Loss/Train_Epoch', train_loss, epoch)\n",
    "\n",
    "        #Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for sample in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Validation]\"):\n",
    "                rgb = sample['image']\n",
    "                depth = sample['depth']\n",
    "                rgb = rgb.to(DEVICE)\n",
    "                depth = depth.to(DEVICE)\n",
    "\n",
    "                pred = model(rgb)\n",
    "                loss = loss_fn(pred, depth)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        writer.add_scalar('Loss/Validation_Epoch', val_loss, epoch)\n",
    "\n",
    "        # Save the model if validation loss improves\n",
    "        if not os.path.exists(CHECKPOINT_DIR):\n",
    "            os.makedirs(CHECKPOINT_DIR)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Move the previous best checkpoint to a backup location\n",
    "            if os.path.exists(BEST_CHECKPOINT_PATH):\n",
    "                shutil.move(BEST_CHECKPOINT_PATH, BACKUP_CHECKPOINT_PATH)\n",
    "\n",
    "            torch.save(model.state_dict(), BEST_CHECKPOINT_PATH)\n",
    "            print(f\"Model saved to {BEST_CHECKPOINT_PATH}\")\n",
    "            print(f\"Previous best model moved to {BACKUP_CHECKPOINT_PATH}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Patience counter: {patience_counter}/{patience}\")\n",
    "        \n",
    "        # Save a sample image and depth map for visualization\n",
    "\n",
    "        # Load a sample from the training set\n",
    "        rgb = cv2.imread(os.path.join(DATA_DIR, \"sample_000000_rgb.png\"))\n",
    "        gt = np.load(os.path.join(DATA_DIR, \"sample_000000_depth.npy\"))\n",
    "\n",
    "        # set the model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            pred = model.infer_image(rgb)\n",
    "        d_min = np.min(pred)\n",
    "        d_max = np.max(pred)\n",
    "\n",
    "        depth_vis = (pred - d_min) / (d_max - d_min + 1e-6)\n",
    "        \n",
    "        cmap = matplotlib.colormaps.get_cmap('plasma')\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.title('Input Image')\n",
    "        plt.axis('off')\n",
    "        plt.imshow(rgb)\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.title('Predicted Depth Map')\n",
    "        plt.axis('off')\n",
    "\n",
    "\n",
    "\n",
    "        plt.imshow(depth_vis, cmap=cmap)\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.title('Ground Truth')\n",
    "        plt.axis('off')\n",
    "        plt.imshow(gt, cmap=cmap)\n",
    "        plt.savefig(os.path.join(RESULT_DIR, f\"epoch_{epoch+1}_sample.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CIL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
